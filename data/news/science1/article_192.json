{
    "thread": {
        "uuid": "7ccf93698ef4c804829577adf54e145bbb0c485a",
        "url": "https://levelup.gitconnected.com/dask-for-scalable-data-science-a-practical-exploration-6cf1dd9dce64",
        "site_full": "levelup.gitconnected.com",
        "site": "gitconnected.com",
        "site_section": "https://levelup.gitconnected.com",
        "site_categories": [
            "javascript",
            "tech"
        ],
        "section_title": "Level Up Coding",
        "title": "Dask for Scalable Data Science: A Practical Exploration | by Okan Yenigün | Dec, 2023 | Level Up Coding",
        "title_full": "Dask for Scalable Data Science: A Practical Exploration | by Okan Yenigün | Dec, 2023 | Level Up Coding",
        "published": "2023-12-28T22:20:00.000+02:00",
        "replies_count": 0,
        "participants_count": 1,
        "site_type": "news",
        "country": "US",
        "main_image": "https://miro.medium.com/v2/resize:fit:1200/1*IVGZpfpYg7e-t4THAFkZLg.png",
        "performance_score": 0,
        "domain_rank": 19196,
        "domain_rank_updated": "2023-12-26T12:06:20.000+02:00",
        "reach": null,
        "social": {
            "facebook": {
                "likes": 0,
                "comments": 0,
                "shares": 0
            },
            "gplus": {
                "shares": 0
            },
            "pinterest": {
                "shares": 0
            },
            "linkedin": {
                "shares": 0
            },
            "stumbledupon": {
                "shares": 0
            },
            "vk": {
                "shares": 0
            }
        }
    },
    "uuid": "7ccf93698ef4c804829577adf54e145bbb0c485a",
    "url": "https://levelup.gitconnected.com/dask-for-scalable-data-science-a-practical-exploration-6cf1dd9dce64",
    "ord_in_thread": 0,
    "parent_url": null,
    "author": "Okan Yenigün",
    "published": "2023-12-28T22:20:00.000+02:00",
    "title": "Dask for Scalable Data Science: A Practical Exploration | by Okan Yenigün | Dec, 2023 | Level Up Coding",
    "text": "Dask for Scalable Data Science: A Practical Exploration Mastering Parallel Computing in Python with Dask: A Comprehensive Guide Dask is a powerful open-source Python library for parallel and distributed computing. It allows us to scale Python code from the local machine’s multi-core CPU to large distributed clusters in the cloud. [Here, you can visit the official page.](https://www.dask.org) pip install dask I generated a dummy dataset using the below code to use it in my demonstrations. import pandas as pd import numpy as np def create_dataset(num_rows): # Generate data data = { \"Product_ID\": [f\"P{str(i).zfill(4)}\" for i in range(1, num_rows + 1)], \"Category\": np.random.choice([\"Electronics\", \"Clothing\", \"Home Appliances\", \"Books\", \"Toys\"], size=num_rows), \"Brand\": np.random.choice([\"BrandA\", \"BrandB\", \"BrandC\", \"BrandD\", \"BrandE\"], size=num_rows), \"Store_Location\": np.random.choice([\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Miami\"], size=num_rows), \"Stock_Status\": np.random.choice([\"In Stock\", \"Out of Stock\", \"Backorder\"], size=num_rows), \"Shipping_Option\": np.random.choice([\"Standard\", \"Express\", \"Two-Day\", \"Overnight\"], size=num_rows), \"Customer_Rating\": np.random.uniform(1, 5, size=num_rows).round(2), \"Warranty_Years\": np.random.choice([1, 2, 3, 5], size=num_rows), \"Product_Weight_kg\": np.random.uniform(0.5, 25, size=num_rows).round(2), \"Sales_Volume\": np.random.randint(100, 1 , size=num_rows), \"Return_Rate\": np.random.uniform(0.01, 0.20, size=num_rows).round(2), \"Profit_Margin\": np.random.uniform(5.00, 50.00, size=num_rows).round(2), \"Average_Delivery_Days\": np.random.randint(1, 30, size=num_rows), \"Supplier_Distance_km\": np.random.uniform(50, 10000, size=num_rows).round(2), \"Manufacturing_Cost\": np.random.uniform(10, 500, size=num_rows).round(2), \"Discount_Rate\": np.random.uniform(0, 0.30, size=num_rows).round(2), \"Year_Released\": np.random.randint(2000, 2023, size=num_rows), \"Price\": np.random.uniform(20, 2000, size=num_rows).round(2), \"Market_Share\": np.random.uniform(0.01, 0.50, size=num_rows).round(2), \"Product_Age_Days\": np.random.randint(1, 1000, size=num_rows) } df = pd.DataFrame(data) return df - Why and When Do We Prefer Dask - Client - Arrays - Lazy Evaluation - Delayed - Futures - DataFrame - Bags - Data Persisting - Distributed Computing Why and When Do We Prefer Dask? When data exceeds the capacity of a machine’s memory, Dask is preferred as it divides the data into manageable chunks for parallel processing, either on a single machine or across a cluster. When facing slow code execution, especially with large-scale data processing, Dask is preferred for its ability to parallelize computations. By distributing tasks across multiple CPU cores or different machines in a cluster, Dask significantly speeds up execution times compared to traditional single-threaded approaches. Client The Client class provides a simple and accessible interface to a Dask cluster. It connects ourw Python session to a Dask cluster. This cluster could be running on your local machine (a local cluster) or spread across multiple machines in a data center or cloud environment. from dask.distributed import Client client = Client() - Address: Specifies the address of the Scheduler in the cluster. If not provided, the Client will create a LocalCluster. If you have a pre-existing Dask cluster, you can use its scheduler’s address. client = Client('127.0.0.1:8786') # Connect to a specific scheduler - n_workers: Number of worker processes to start. Ideally, set this to the number of cores available if you’re running on a single machine. For distributed clusters, this will depend on the number of machines and cores per machine in your cluster. client = Client(n_workers=8) # Start a cluster with 8 workers - threads_per_worker: Number of threads per each worker process. This often depends on the type of tasks. CPU-bound tasks might benefit from 1–2 threads per core, while I/O-bound tasks can use more. A good starting point is to set it equal to the number of cores per worker. client = Client(threads_per_worker=8) - processes: Whether to use processes ( True) or threads ( False). By default, Dask uses processes. Use processes (default) for CPU-bound tasks, as they avoid the Global Interpreter Lock (GIL) in Python. Use threads for I/O-bound tasks or when memory sharing between tasks is more important. client = Client(processes=False) # Use threads instead of processes - memory_limit: he maximum amount of memory that each worker can use. You can specify this as an integer (bytes), a string (like ‘2GB’), or None(no limit). Set this based on the available RAM and the memory requirements of your tasks. As a rule of thumb, leave some buffer and don’t allocate all available memory to Dask workers. client = Client(memory_limit='4GB') - silence_logs: The logging level. This can be useful to reduce or increase the verbosity of logs. import logging client = Client(silence_logs=logging.ERROR) - asynchronous: If set to True, allows the Client to be used in asynchronous programming. client = Client(asynchronous=True) - dashboard_address: Specifies the address for the dashboard, which provides a real-time web UI to monitor the cluster. Set to Noneto disable. client = Client(dashboard_address=':8787') - name: An optional name to give the cluster, making it easier to identify. client = Client(name='MyDaskCluster') - scheduler_file: Path to a file with scheduler information if you’re connecting to an existing cluster. client = Client(scheduler_file='/path/to/scheduler.json') - security: For secure connections, especially in a distributed setup. from dask.distributed import Security security = Security(tls_ca_file='ca.pem', tls_worker_cert='worker.pem', ...) client = Client(security=security) DASK provides a dashboard to monitor what is happening in the system behind the scenes. Arrays Dask arrays are composed of many small NumPy arrays, organized into a grid. These small arrays are known as “chunks.” By breaking a large array into smaller chunks, Dask can work on each chunk independently. This chunked structure allows for parallel and distributed computing. By processing data in chunks, Dask can work with very large arrays that don’t fit entirely in RAM, only loading chunks into memory as needed. This is called block algorithms. Block algorithms are a method of processing large datasets by breaking them down into smaller, more manageable pieces, or “blocks”. - The data is divided into smaller chunks or blocks. - Each block can be processed independently. - After processing, the results from individual blocks are aggregated or combined to form the final result. Dask arrays are an implementation of block algorithms for handling large, multi-dimensional array data. [Read the original paper here.](https://conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf) Let’s create a very large array: import dask.array as da dask_array = da.random.random(size=(1 , 1 ), chunks=(10000, 10000)) dask_array A 74.5 GB array is far too large for my regular laptop to hold in its memory. Nevertheless, by utilizing DASK’s chunking capabilities, we can manage and perform operations on this massive array. %%time dask_sum = dask_array.sum() dask_sum_computed = dask_sum.compute() dask_sum_computed \"\"\" CPU times: user 58.5 s, sys: 13.6 s, total: 1min 12s Wall time: 11.4 s 4999930574.208597 \"\"\" Lazy Evaluation In lazy evaluation, the evaluation of expressions is delayed until their values are actually needed. When you write an expression or call a function, it doesn’t compute the result right away. Instead, it stores a representation of the expression, often in the form of a task or a promise to compute it later. In Python lazy evaluation can be implemented using features like generators, iterators, or special library functions. In eager evaluation, the computation is performed immediately. def eager_evaluation(numbers): squared_numbers = [x**2 for x in numbers] return squared_numbers numbers = [1, 2, 3, 4, 5] squared = eager_evaluation(numbers) print(\"Eager Evaluation:\", squared) # Eager Evaluation: [1, 4, 9, 16, 25] For lazy evaluation, I’ll use a generator. A generator computes the values on the fly and yields them one at a time, as they are requested, rather than all at once. def lazy_evaluation(numbers): for x in numbers: yield x**2 numbers = [1, 2, 3, 4, 5] lazy_squared = lazy_evaluation(numbers) print(lazy_squared) # print(\"Lazy Evaluation:\") for number in lazy_squared: print(number) \"\"\" Lazy Evaluation: 1 4 9 16 25 \"\"\" When we perform operations on Dask arrays, these operations are not executed immediately. Instead, Dask builds a task graph that describes the operations. The actual computations are only executed when we call a method like .compute(). This approach allows Dask to optimize the execution by organizing tasks efficiently and handling large datasets effectively. Delayed Dask’s delayed is a powerful feature used to parallelize existing Python code that wasn't originally designed for parallel execution. Let’s create simple Python functions: from time import sleep def inc(x): sleep(1) return x + 1 def add(x, y): sleep(1) return x + y Run it sequentially: %%time x = inc(1) y = inc(2) z = add(x, y) \"\"\" CPU times: user 517 µs, sys: 756 µs, total: 1.27 ms Wall time: 3.01 s \"\"\" 3 function call, 3 seconds. Now, let’s introduce delayed . from dask import delayed x = delayed(inc)(1) y = delayed(inc)(2) z = delayed(add)(x, y) print(z) \"\"\" Delayed('add-49d649b5-3e1f-40db-a36e-3b62b5da9142') \"\"\" We have a Delayed object, lazily evaluated. %%time result = z.compute() \"\"\" CPU times: user 1.94 ms, sys: 1.48 ms, total: 3.42 ms Wall time: 2.01 s \"\"\" We gain one seconds here. Two inc function run in parallel. z.visualize() We can also use delayed on for-loops: datasets = list(range(1,9)) results = [] for data in datasets: result = delayed(inc)(data) results.append(result) total = delayed(sum)(results) total \"\"\" Delayed('add-fbd29c44-a55d-43da-8c46-67f047d0d7cf') \"\"\" %%time total.compute() \"\"\" CPU times: user 1.15 ms, sys: 1.07 ms, total: 2.22 ms Wall time: 1.01 s 44 \"\"\" total.visualize() Eight inc function run in parallel. We can also use delayed decorator on top of functions. @delayed def square(num): sleep(1) return num * num @delayed def sum_list(numbers): sleep(1) return sum(numbers) The decorator transforms the function so that when we call it, instead of executing immediately, it returns a delayed object. data = [1, 2, 3, 4, 5] squared_data = [square(x) for x in data] total = sum_list(squared_data) total \"\"\" Delayed('sum_list-d665a59f-1d88-4444-bac9-05b2ef75fb01') \"\"\" %%time result = total.compute() result \"\"\" CPU times: user 1.76 ms, sys: 1.28 ms, total: 3.04 ms Wall time: 2.01 s 55 \"\"\" total.visualize() Futures Dask Futures offer a powerful way to handle asynchronous computation, similar to the concept of futures in concurrent programming. We submit a task (function and arguments) to a Dask scheduler using methods like client.submit(). The scheduler then dispatches these tasks to available workers. from dask.distributed import Client client = Client(n_workers=4) def square(x): return x * x future = client.submit(square, 4) future \"\"\" Future: square status: pending, type: NoneType, key: square-94301df5f4e518f1fc5e940b73b9dce4 \"\"\" result = future.result() print(result) # 16 values = [1, 2, 3, 4, 5] futures = [client.submit(square, value) for value in values] futures \"\"\" [ , , , , ] \"\"\" results = client.gather(futures) print(results) \"\"\" [1, 4, 9, 16, 25] \"\"\" The gather call will block until all computations are finished and return the results. The submit method has an optional parameter called pure. In the context of programming, a function is considered “pure” if it always produces the same output given the same input and has no side effects. This means that the function doesn’t modify any external state (like global variables, I/O operations, etc.) and its return value depends only on its input arguments. In Dask, if a function is marked as pure, it allows Dask to cache the results. If the same function with the same arguments is submitted again, Dask can reuse the previously computed result instead of recomputing it. import random def add_random_number(a): return a + random.randint(1, 100) future3 = client.submit(add_random_number, 5, pure=False) future4 = client.submit(add_random_number, 5, pure=False) # Check if the futures point to the same computation print(future3.key == future4.key) # Output: False def add_numbers(a, b): return a + b future1 = client.submit(add_numbers, 4, 5, pure=True) future2 = client.submit(add_numbers, 4, 5, pure=True) # Check if the futures point to the same computation print(future1.key == future2.key) # Output: True Since add_numbers is pure and we've set pure=True, Dask knows it can cache the result. The two futures ( future1 and future2) reference the same computation because Dask recognizes that the function will produce the same output for the same inputs. Additionally, we can use map method: def square(x): return x * x futures = client.map(square, range(1, 9)) res = client.gather(futures) res \"\"\" [1, 4, 9, 16, 25, 36, 49, 64] \"\"\" The wait function is a crucial tool used in conjunction with asynchronous computation. It is used to pause the execution of your program until specified futures (or a single future) have completed their computation. # Submit the first task to Dask: future1 = client.submit(first_function, 10) from dask.distributed import wait # Wait for the first future to complete wait([future1]) # Submit the second task # Note: You don't need to call .result() here; Dask handles the dependency future2 = client.submit(second_function, future1) result = future2.result() print(\"Result of the second function:\", result) # Result of the second function:: 22 first_functionis executed with an input of 10. - The program waits until first_functionis completed. - Once first_functionis finished, its output is automatically used as the input for second_function. - The result of second_functionis then retrieved. The as_completed function provides a way to iterate over futures as they finish, regardless of the order in which they were submitted. as_completed takes an iterable of Future objects and returns an iterator that yields each future as it completes. This function is especially useful when you have multiple independent tasks and you want to start processing results as soon as each task completes, rather than waiting for all tasks to finish. from dask.distributed import as_completed # Define a simple task function def my_task(x): # Some computations return x * 2 # Submit multiple tasks futures = [client.submit(my_task, i) for i in range(5)] # Create an as_completed iterator completed_futures = as_completed(futures) # Process each future as it completes for future in completed_futures: result = future.result() print(\"Task completed with result:\", result) client.close() \"\"\" Task completed with result: 0 Task completed with result: 2 Task completed with result: 4 Task completed with result: 6 Task completed with result: 8 \"\"\" DataFrame A Dask Dataframe is composed of multiple smaller Pandas Dataframes, partitioned along the index. Each partition is a valid Pandas Dataframe that fits into memory. Dask Dataframes aim to mimic the Pandas API as closely as possible, making it familiar to Pandas users. Most common Pandas operations (like groupby, join, and time series computations) are available in Dask. Let’s start with reading a CSV file: import dask.dataframe as dd ddf = dd.read_csv(\"data.csv\", blocksize=\"1GB\") ddf It just returned a schema of the dataframe (lazy evaluation), column names, data types, and the number of partitions. Each partition has 1 GB of data. print(ddf.columns) \"\"\" Index(['Product_ID', 'Category', 'Brand', 'Store_Location', 'Stock_Status', 'Shipping_Option', 'Customer_Rating', 'Warranty_Years', 'Product_Weight_kg', 'Sales_Volume', 'Return_Rate', 'Profit_Margin', 'Average_Delivery_Days', 'Supplier_Distance_km', 'Manufacturing_Cost', 'Discount_Rate', 'Year_Released', 'Price', 'Market_Share', 'Product_Age_Days'], dtype='object') \"\"\" print(ddf.dtypes) \"\"\" Product_ID string[pyarrow] Category string[pyarrow] Brand string[pyarrow] Store_Location string[pyarrow] Stock_Status string[pyarrow] Shipping_Option string[pyarrow] Customer_Rating float64 Warranty_Years int64 Product_Weight_kg float64 Sales_Volume int64 Return_Rate float64 Profit_Margin float64 Average_Delivery_Days int64 Supplier_Distance_km float64 Manufacturing_Cost float64 Discount_Rate float64 Year_Released int64 Price float64 Market_Share float64 Product_Age_Days int64 dtype: object \"\"\" print(ddf.npartitions) \"\"\" 6 \"\"\" Dask syntax is a lot similar to Pandas. min_distance = ddf[\"Supplier_Distance_km\"].min() print(min_distance) #dd.Scalar min() doesn't compute its results immediately. Instead, it builds a task graph describing the computation. It returns a representation, a placeholder of the result. Now let’s visualize the task graph for min_distance min_distance.visualize() Task graphs in Dask are interpreted from the bottom upwards. Starting at the bottom, there are six instances of reading CSV files. Following this, for each partition, the ‘Supplier_Distance_km’ column is extracted. Subsequently, it computes the minimum value for this column. Ultimately, it aggregates the six distinct minimum values to determine the final outcome. Up to this point, Dask has not executed any actual computation; it has merely constructed the task graph To execute any task graph, we use the compute method. min_distance.compute() # 50.04 We can use groupby. ddf.groupby(\"Stock_Status\")[\"Stock_Status\"].count().compute() \"\"\" Stock_Status Backorder 333258 In Stock 333937 Out of Stock 332805 Name: Stock_Status, dtype: int64 \"\"\" map_partitions allows us to apply a custom function to each partition of a Dask collection independently. def multiply_by_two(df): return df.apply(lambda x: x * 2) def multiply_by_n(df, n): return df.apply(lambda x: x * n) ddf[\"Adjusted_Average_Delivery_Days\"] = ddf[\"Average_Delivery_Days\"].map_partitions(multiply_by_two) ddf[\"Adjusted_Average_Delivery_Days_3\"] = ddf[\"Average_Delivery_Days\"].map_partitions(multiply_by_n, 3) ddf[[\"Average_Delivery_Days\",\"Adjusted_Average_Delivery_Days\", \"Adjusted_Average_Delivery_Days_3\"]].head() When using compute it is always better to look for optimization opportunities. mean_delivery_days = ddf[\"Average_Delivery_Days\"].mean() std_delivery_days = ddf[\"Average_Delivery_Days\"].std() mean_delivery_result = mean_delivery_days.compute() std_delivery_result = std_delivery_days.compute() mean_delivery_result, std_delivery_result \"\"\" Took ~3 seconds (15.003243, 8.36907297879123) \"\"\" import dask mean_delivery_result, std_delivery_result = dask.compute(mean_delivery_days, std_delivery_days) mean_delivery_result, std_delivery_result \"\"\" Took ~1.5 seconds \"\"\" dask.visualize(mean_delivery_days, std_delivery_days) Results of tasks are being shared. Bags Dask Bags are a component designed to efficiently handle unstructured or semi-structured data, similar to what we might find in formats like JSON, logs, or text data. They are particularly well-suited for operations on data that don’t fit neatly into tabular structures like arrays or dataframes. Bags are often used for initial data processing, cleaning, and transformation before converting data into more structured formats for detailed analysis. import dask.bag as db file_pattern = 'feedback*.txt' b = db.read_text(file_pattern) def count_keyword(line, keyword='excellent'): return line.lower().count(keyword) keyword_counts = b.map(lambda line: count_keyword(line)).sum() total_count = keyword_counts.compute() Data Persisting Data persisting refers to the practice of saving data in a state that can be retrieved and used even after the process that created or modified it has ended. Dask allows for the persistence of data in memory across distributed workers. This is particularly useful when you have intermediate results that are computationally expensive and will be used multiple times in subsequent operations. ddf_persisted = ddf.persist() Distributed Computing Distributed computing is the ability to execute parallel computations across multiple machines in a cluster, making it a powerful tool for handling large-scale data processing and analysis tasks. Dask’s distributed computing capability is managed by the Dask distributed system, a lightweight and flexible library for distributed computing in Python. It extends Dask’s concurrency model to span across multiple machines in a cluster. The heart of the system is the scheduler. It manages and coordinates the execution of tasks across all the workers. Workers are the processes that actually execute the tasks and store the results. Each worker can process multiple tasks concurrently. Client is the user-facing entry point for submitting tasks to the cluster. The client communicates with the scheduler, which in turn assigns tasks to workers. Conclusion In conclusion, Dask offers a powerful and flexible solution for scaling up Python computations and handling large datasets. By providing tools for parallel processing, lazy evaluation, and distributed computing, it enables efficient and effective data analysis workflows that are both user-friendly and highly adaptable to complex, large-scale data challenges. One Sentence To Remember: Dask is preferred for processing data that exceeds a machine’s memory capacity and for accelerating slow code execution. Read More Optimization of Pandas Performance on Large Data Ways to Speed Up Performance in Pandas Package python.plainenglish.io Buckle Up Your For-Loops, Or How to Speed Up For Loops in Python Faster Alternatives to Python’s Standard For Loop Implementations python.plainenglish.io Exploring Hugging Face: Table Question Answering Table Question Answering with Transformer Models levelup.gitconnected.com Sources [https://www.youtube.com/watch?v=_2jKSo0GCI8&list=PLTgRMOcmRb3OlkfAdqJWyGGrQM7eU-mi7&index=2](https://www.youtube.com/watch?v=_2jKSo0GCI8&list=PLTgRMOcmRb3OlkfAdqJWyGGrQM7eU-mi7&index=2) [https://conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf](https://conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf) [https://www.youtube.com/watch?v=z18qjLu-Mw4&list=PLeDTMczuyDQ8S73cdc0PrnTO80kfzpgz2&index=1](https://www.youtube.com/watch?v=z18qjLu-Mw4&list=PLeDTMczuyDQ8S73cdc0PrnTO80kfzpgz2&index=1) [https://www.youtube.com/watch?v=32w33L7hseQ](https://www.youtube.com/watch?v=32w33L7hseQ) [https://www.youtube.com/watch?v=8bd7DswSxw4](https://www.youtube.com/watch?v=8bd7DswSxw4)",
    "highlightText": "",
    "highlightTitle": "",
    "highlightThreadTitle": "",
    "language": "english",
    "sentiment": "positive",
    "categories": [
        "Science and Technology",
        "Education"
    ],
    "external_links": [
        "http://www.youtube.com/watch?v=32w33L7hseQ",
        "http://www.youtube.com/watch?v=32w33L7hseQ)",
        "http://www.youtube.com/watch?v=z18qjLu-Mw4",
        "http://www.youtube.com/watch?v=8bd7DswSxw4",
        "https://conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf)",
        "http://www.youtube.com/watch?v=8bd7DswSxw4)",
        "https://conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf",
        "https://www.dask.org)",
        "http://www.youtube.com/watch?v=_2jKSo0GCI8",
        "https://www.conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf",
        "http://youtube.com/watch?v=z18qjLu-Mw4",
        "http://youtube.com/watch?v=8bd7DswSxw4",
        "http://youtube.com/watch?v=8bd7DswSxw4)",
        "http://youtube.com/watch?v=_2jKSo0GCI8",
        "http://youtube.com/watch?v=32w33L7hseQ",
        "https://www.conference.scipy.org/proceedings/scipy2015/pdfs/matthew_rocklin.pdf)",
        "http://youtube.com/watch?v=32w33L7hseQ)",
        "http://www.youtube.com/watch",
        "https://dask.org)"
    ],
    "external_images": [],
    "entities": {
        "persons": [
            {
                "name": "okan yenigün",
                "sentiment": "negative"
            },
            {
                "name": "dask",
                "sentiment": "negative"
            }
        ],
        "organizations": [],
        "locations": [
            {
                "name": "new york",
                "sentiment": "none"
            },
            {
                "name": "chicago",
                "sentiment": "none"
            },
            {
                "name": "los angeles",
                "sentiment": "none"
            },
            {
                "name": "houston",
                "sentiment": "none"
            },
            {
                "name": "miami",
                "sentiment": "none"
            }
        ]
    },
    "rating": null,
    "crawled": "2023-12-29T00:25:31.542+02:00",
    "updated": "2023-12-29T00:25:31.542+02:00"
}